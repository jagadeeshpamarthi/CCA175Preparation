Standard Transformations :
 String manipulations
 Row level Transformations
 Filtering (Horizontal,vertical)
 Joins
 Aggregations
 sorting
 Ranking
 set operations

Transformation API differentiation : 
  map,flatMap,mapPartition,
  mapPartitionWithIndex     -> Row Level Transformation
  filter                    -> to Filter
  reduceBy,aggregateBy      -> aggregation
  union,intersection,distinct -> set operations
  groupBy                     ->Ranking
  joins                       ->joins
  sortBy                      -> sorting
  
  
  

1 String Manipulations : 
    similar to java string functions
    val str = "hello how r u"
    str.substring(0,10)            //sub string
    str.contains("hello")         //boolean
    str.length
    if(str == "hello how r u")
    str.split(",")                  imp
    etc
    
2  Row level Transformations
    Map:-
    (fun : T => U):map[U]
    takes one argument and returns single value RDD as output
    flatMap :- similar to map , but it will return one or more values as output
    ex:
    val str = List("hello","this is word count","to count word")
    val strRDD = sc.parallelize(str)
    val strFlatMap = strRDD.flatMap(str => str.split(" "))
    val strMap = strFlatMap.map(word => (word,"")).countByKey
    
3 Flitering (horzontal)
    filter API is used for horizontal transformation
    map is used for vertical(Row)
    
4 Joins :
    > join(otherDataset, [numTasks]) ->	When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with
    all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.
    > In order to perform joins the data sets must be pairredRDDs(tupels) and the key value of both data sets should be matching
    >ex :
    joining order and order_items :
      val orders = sc.textFile("path")
      val order_items = sc.textFile("path")
      val orderMap = orders.map(order => (order.split(",")(0).toInt,order.split(",")(1).substring(0,10))
      val orderItemsMap = order_items.map(order => (order.split(",")(1).toInt,order.split(",")(3))
      val orderJoin = orderMap.join(orderItemsMap)                                                       //Inner join  (output contains only if record matches)
      
      outerJoins :
      ex : get all orders which do not have corresponding entries in order_items table
      val orders = sc.textFile("path")
      val order_items = sc.textFile("path")
      val orderMap = orders.map(order => (order.split(",")(0).toInt,order)
      val orderItemsMap = order_items.map(order => (order.split(",")(1).toInt,order)
      val orderLeftOuterJoin = orderMap.leftOuterJoin(orderItemsMap)                                 //leftOuter join  (output contains only if record matches)
      orderLeftOuterJoin.take(10).foreach(println)
      (1,(1,2013-09,complete,some(1,1,299))
      (2,(2,2013-09,complete,some(1,2,4299))
      (3,(3,2013-09,complete,some(1,3,2299))      
      (4,(4,2013-09,complete,None)
      (5,(5,2013-09,complete,None)      
      5 more records...
      from the above data set we need only the records which are None
      val noneValues = orderLeftOuterJoin.filter(noneV => {
                                                noneV._2._2 == "None" 
                                                })
      val finalOrders = noneValues.map(orders => orders._2._1)
      final output is finalOrders.
      rightOuterJoin is same as leftOuterJoin only difference is placing dataset
      val orderRightOuterJoin = orderItemsMap.rightOuterJoin(orderMap)                                 //rightOuter join  (output contains only if record matches)
      full outer joins is used in many to many condition
  
5 Aggregations :
   a cluster of things that have come or been brought together.(sum,min,max ,avg)
   API's those are used for Aggregations are
   Transformation API's : 
     groupByKey
     reduceByKey
     aggregateByKey
   Action API's :                        //global aggregation
      reduce
      countByKey
      
  Excercise : 
     Action API's
       reduce :
           val actionReduce = orders.reduce(total , reduce => total+reduce)
       countByKey:
           val actionCountByKey = orders.map(order => (order.split(",")(3),"")).countByKey.foreach(println)
      
     Transformation API's :
           In order to understand aggregations of Transformation API's we need to understand combainer 1st
              Combainer :
                  combainer is utilizing the resources more efficiently(divide n calculate)
                  GroupByKey : 
                      this API is not suggeseted for aggregation functions like sum ,avg ,max ect (reduceBykey,aggregateByKey suggested)
                      because it dosn't use combainer logic
                  ReduceByKey  :
                      it follows combainer logic , but the logic for intermidate and final combination is same
                      the above case won't work is some situations like :- avg
                      internal partition
                  aggregateByKey  :
                      combianer logic
                      different logic for intermidate and final result
                  example :
                     val sumGroupByKey = sum(1 to 1000) => 1+2+3+ .....+1000
                     val sumReduceByKey = sum(1 to 1000) => (sum(1, 250),sum(251,500),sum(501,750),sum(451,1000))  //combainer
             groupByKey([numTasks]) 	When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.
                                     Note: If you are grouping in order to perform an aggregation (such as a sum or average) 
                                     over each key, using reduceByKey or aggregateByKey will yield much better performance.
                                     Note: By default, the level of parallelism in the output depends on the number of partitions 
                                     of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.
             reduceByKey(func, [numTasks]) 	When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where 
                                           the values for each key are aggregated using the given reduce function func, which
                                           must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable 
                                           through an optional second argument.
             aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) 	When called on a dataset of (K, V) pairs, returns a dataset of 
                                                              (K, U) pairs where the values for each key are aggregated using the given 
                                                              combine functions and a neutral "zero" value. Allows an aggregated value 
                                                              type that is different than the input value type, while avoiding unnecessary 
                                                              allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.
       ex:
        groupByKey : 
          val orderitemsMap = order_items.map(o => (o.split(",")(1).toInt, o.split(",")(4).toFloat)
          val oiMBK = orderitemsMap.groupByKey                                         //No function,requirs pairred RDD as input
          scala> oiMBK.take(10).foreach(println)
              (1,comp...[199,299,33.9]
          
        //aggregation - reduceByKey : 
          val order_items = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/order_items/")
          //get orderitems_id as key and revenue as value , so that we can perform reduceByKey operation to calculate the revenue for particular orderId
          val orderitemMap = order_items.map(order => (order.split(",")(1).toInt,order.split(",")(4).toFloat))
          val orderitemRevenueTotal = orderitemMap.reduceByKey(_+_)
          orderitemRevenueTotal.take(10).foreach(println)
          // TO get minimum value of orderId
          val orderitemRevenueMin = orderitemMap.reduceByKey((min,revenue) => if(min>revenue) revenue else min)

       //aggregation - aggregateByKey
          val order_items = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/order_items/")
          //get orderitems_id as key and revenue as value , so that we can perform aggregateByKey operation to calculate the revenue & min value for  orderId
          val orderitemMap = order_items.map(order => (order.split(",")(1).toInt,order.split(",")(4).toFloat))
          //output : (456,2999.99)
         //         (345,277.98) ...			
         //aggregateByKey syntax. aggregateByKey(initialValues and also desired output format)(2 funtions ,one is for intermediate n 2 is for final)
         val orderitemRevenueTotal = orderitemMap.aggregateByKey((0.0f,0.0f))(  
        //1st function,performed on each partion(node) with intial zero values and key's VALUE as input,returns output which is tuple(sum,max)                
        (initialTuple,revenue) => (initialTuple._1+revenue,if(initialTuple._2>revenue)initialTuple._2 else revenue),
        // 2nd funtion ,combainer function performed on cluster level taking input as outputs of each node and performing final out put on it
        (nodeN,nodeNplus1) => (nodeN._1+nodeNplus1._1,if(nodeN._2>nodeNplus1._2)nodeN._2 else nodeNplus1._2)
         )
        orderitemRevenueTotal.take(10).foreach(println)
        
      //Sorting - sortByKey
        val product = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/product/")
        //get productsId & subtotal as Compositekey and product as value , so that we can perform sortByKey operation to sort it with product acsending & //-subtotal decending
        val productMap = product.
                filter(product => product.split(",")(4) != "").
                map(product => ((product.split(",")(1).toInt, -product.split(",")(4).toFloat),product)).
                sortByKey()
         //getting only sorted values leaving unnececcary values apart
         val sortedProducts = productMap.map(product => product._2)
         
      //Ranking  takeOrdered
         val product = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/product/")
         //simple way to get top N products with price in decending order using takeOrdered ACTION 
         val productMap = product.
                filter(product => product.split(",")(4) != "").
                takeOrdered(10)(Ordering[Float].reverse.on(product => product.split(",")(4).toFloat)).foreach(println)
   
 Excercise 1 :   
  //Ranking -Get top N priced products with in each product category
    val product = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/product/")
    val productMap = product.
                filter(product => product.split(",")(4) != "").
                map(product => (product.split(",")(1).toInt, product))
    val productsGBK = productMap.groupByKey
    //productsGBK: org.apache.spark.rdd.RDD[(Int, Iterable[String])] = ShuffledRDD[18] at groupByKey at <console>:28
    //output:
    //-(30,CompactBuffer(645,30,Roxy Women's Steamer Long Sleeve Rash Guard,,68.0,http://images.acmesports.sports/    //-Roxy+Women%27s+Steamer+Long+Sleeve+Rash+Guard, 646,30,Merrell Women's Grassbow Sport Hiking Shoe,,99.99,http://images.acmesports.sports/   //-Merrell+Women%27s+Grassbow+Sport+Hiking+Shoe, 647,30,Merrell Women's Siren Mid Waterproof Hiking B,,134.99,http://images.acmesports.sports/      //-Merrell+Women%27s+Siren+Mid+Waterproof+Hiking+Boot, ...))
    val productsGBKFirstIterable = productsGBK.first._2

    def topNPricedProducts(productIterable:Iterable[String],topN:Int):Iterable[String] = {
        val productsSorted = productIterable.toList.sortBy(product => -product.split(",")(4).toFloat)
        val topNPricesSet = productIterable.map(product=>product.split(",")(4).toFloat).toSet
        val topNPrices = topNPricesSet.toList.sortBy(p => -p).take(topN)
        val topNpriceMin = topNPrices.min
        val finalSortedProducts = productsSorted.takeWhile(product => product.split(",")(4).toFloat >= topNpriceMin)
        finalSortedProducts
    }
    val finalResultProducts = productsGBK.map(product => topNPricedProducts(product._2,1))

Set Operations : 
union(otherDataset) 	       Return a new dataset that contains the union of the elements in the source dataset and the argument.
intersection(otherDataset) 	Return a new RDD that contains the intersection of elements in the source dataset and the argument.
distinct([numPartitions])) 	Return a new dataset that contains the distinct elements of the source dataset.  

val orders = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/orders/")
    val orders_2018_08 = orders.filter(order => order.split(",")(1).contains("2018-08")).
                              map(order => order.split(",")(2))
    val orders_2018_09 = orders.filter(order => order.split(",")(1).contains("2018-09")).
                              map(order => order.split(",")(2))
     val orders_2018_08_intersection_09 =  orders_2018_08.intersection(orders_2018_09)
     val orders_2018_08_union_09 =  orders_2018_08.union(orders_2018_09).distint
   
saveAsTextFile :   
saveAsTextFile(path) 	Write the elements of the dataset as a text file (or set of text files) in a given directory in the 
                      local filesystem, HDFS or any other Hadoop-supported file system. 
                      Spark will call toString on each element to convert it to a line of text in the file.   
     Syntax :
        val saveRDD =  orders_2018_08_union_09.saveAsTextFile("/path")   
  2 types : 
     val saveRDD =  orders_2018_08_union_09.saveAsTextFile("/path")      
     val saveRDD =  orders_2018_08_union_09.saveAsTextFile("/path",classOf[org.apache.hadoop.io.compress.snappycodec])     //standard formates  //gZip,Bzip,snappy,defalutCompress etc"     

save in other file formats :
    val loadDF = sqlContext.load("/path","foramat : jason/paraqute/orc/avro")
    val readDF = sqlContext.read.json("/path")

    val loadDF = sqlContext.save("/path","foramat : jason/paraqute/orc/avro")
    val writeDF = sqlContext.wirte.json/orc/paraqute("/path")
    
============================================================================================================================================          
Excercise 2 :

  val orders = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/orders/")
  val ordersFilter = orders.filter(order => (order.split(",")(3) == "COMPLETE" || order.split(",")(3) == "CLOSED"))
  val ordersMap = ordersFilter.map(order => (order.split(",")(0).toInt, order.split(",")(1)))
  val order_items = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/order_items/")
  val oiMap = order_items.map(oi => (oi.split(",")(1).toInt,(oi.split(",")(2).toInt,oi.split(",")(4).toFloat)))
  val omJoinOim = ordersMap.join(oiMap)
  val dailyRevenuePerProduct = omJoinOim.map(o => ((o._2._1,o._2._2._1),o._2._2._2)).reduceByKey((r,t)=>r+t)
  val omJoinOimMap = dailyRevenuePerProduct.map(o => (o._1._2,(o._1._1,o._2)))
  val product = sc.textFile("/user/jagadeeshpamarthi35410/retail_db1/product/")
  val pMap = product.map(p => (p.split(",")(0).toInt,p.split(",")(2)))
  val omJoinOimMapJoinpMap = omJoinOimMap.join(pMap)
  val finalMap = omJoinOimMapJoinpMap.map(op => ((op._2._1._1,op._2._1._2),op._2._2))
  val finalMapSorted = finalMap.map(f => ((f._1._1,-f._1._2),f)).sortByKey()
  val finalResult = finalMapSorted.map(f => f._2._1._1+","+f._2._1._2+","+f._2._2)
  val saveexcercise2 = finalResult.saveAsTextFile("/user/jagadeeshpamarthi35410/Excercise2_Result")


    
